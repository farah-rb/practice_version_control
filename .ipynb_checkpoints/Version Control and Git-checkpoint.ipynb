{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0332dbc-6b02-4ee1-8dda-fcca1d17c0d4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#F18A00; padding:10px;\">\n",
    "    <h1 style=\"color:white;\"> Version Control and Git Flow Task</h1>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e938ac8-83f3-4778-bb6e-a88cea030a9b",
   "metadata": {},
   "source": [
    "## Introduction to Version Control and Git Flow\n",
    "\n",
    "Version control is a crucial aspect of software development, enabling teams to collaborate effectively on code, track changes, and manage project changes over time. Git allows multiple developers to work on a project simultaneously, ensuring that changes can be integrated smoothly and conflicts can be resolved efficiently.\n",
    "\n",
    "In this task, we will be utilizing Git and GitHub to manage a pipeline. Between you, decide which ticket will be done by which members of the team, representing a task that needs to be completed. By following the Git flow, we will ensure that all changes are systematically reviewed and integrated into the main codebase.ase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32638306-d2ef-4af5-bebb-3575272ea50f",
   "metadata": {},
   "source": [
    "## Tickets!\n",
    "\n",
    "Please complete the following tickets. Decide which tickets will be split between which team members and work as a team, incorporating the changes into the finalised code\n",
    "\n",
    "- [x] Set up Git ðŸ¥³ \n",
    "- [ ] Reintroduce the 'Final Outcome' column which was removed in the pipeline\n",
    "- [ ] Delete longitude and latitude columns\n",
    "- [ ] Change exported file type (export it as an excel file)\n",
    "- [ ] Change 'Broad Outcome Category' column name to 'Finalised Outcome'\n",
    "- [ ] Remove rows containing bike or car thefts within the crime type\n",
    "- [ ] Split the 'LSOA Name' column into LSOA region and LSOA number (e.g. if 'LSOA Name' is \"Basingstoke and Deane 001A\" then the output should be 'LSOA Region: Basingstoke and Deane' and 'LSOA number:001A')\n",
    "- [ ] Split 'Month' into month and year (e.g. 2022-01 would be 'Year: 2022 and Month: 01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f59ed2-1a17-4481-99c9-86d859006414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#farah has added this line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821691b-e9a4-4bbd-89bf-2bc91773eafe",
   "metadata": {},
   "source": [
    "## Task: Make Changes to the Pipeline and Observe Alterations in the GitHub Repository\n",
    "\n",
    "### 1. Complete Tickets\n",
    "Choose the ticket that you will be working on. Implement changes to the pipeline below (please edit directly on the pipeline as these changes will be visible when uploaded to Git.\n",
    "\n",
    "### 2. Push Changes to the Repository\n",
    "Now that you have made the changes to the pipeline, push it onto a new branch using Git CMD on the repository that was created at the start of the lesson.\n",
    "\n",
    "Save this file.\n",
    "\n",
    "Remember to write detailed descriptions of the changes that you made to the pipeline to make sure that your team knows what has been altered.\n",
    "Push your changes on a separate branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644d5d0-fe9d-450f-809f-6f025ad8101c",
   "metadata": {},
   "source": [
    "### Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd67636-9962-4c6d-aac3-46ed2c676553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "LOCAL_DATA_PATH = './'\n",
    "LOG_FILE = os.path.join(LOCAL_DATA_PATH, 'pipeline.log')\n",
    "RAW_DATA_FILE = os.path.join(LOCAL_DATA_PATH, '2022-01-cheshire-street.csv')\n",
    "OUTCOMES_DATA_FILE = os.path.join(LOCAL_DATA_PATH, '2022-01-cheshire-outcomes.csv')\n",
    "STAGED_DATA_FILE = os.path.join(LOCAL_DATA_PATH, 'staged_cheshire_street.csv')\n",
    "PRIMARY_DATA_FILE = os.path.join(LOCAL_DATA_PATH, 'primary_cheshire_street.csv')\n",
    "REPORTING_DATA_FILE = os.path.join(LOCAL_DATA_PATH, 'reporting_cheshire_street.csv')\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    filemode='a',\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "def ingest_data(file_path):\n",
    "    \"\"\"\n",
    "    Ingest raw data from a CSV file.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting data ingestion from {file_path}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.error(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        logging.info(f\"Data ingestion from {file_path} completed successfully\")\n",
    "        return df\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Error reading the CSV file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def merge_data(df, df_outcomes):\n",
    "    \"\"\"\n",
    "    Merge the main data with outcomes data on 'Crime ID'.\n",
    "    \"\"\"\n",
    "    return pd.merge(df, df_outcomes[['Crime ID', 'Outcome type']], how='left', on='Crime ID')\n",
    "\n",
    "def finaloutcome(df):\n",
    "    \"\"\"\n",
    "    Create 'Final Outcome' column based on 'Outcome type' and 'Last outcome category'.\n",
    "    \"\"\"\n",
    "    df['Final Outcome'] = df.apply(\n",
    "        lambda row: row['Outcome type'] if pd.notnull(row['Outcome type']) else row['Last outcome category'],\n",
    "        axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def categorize_outcome(outcome):\n",
    "    if outcome in ['Unable to prosecute suspect', \n",
    "                   'Investigation complete; no suspect identified', \n",
    "                   'Status update unavailable']:\n",
    "        return 'No Further Action'\n",
    "    elif outcome in ['Local resolution', \n",
    "                     'Offender given a caution', \n",
    "                     'Action to be taken by another organisation', \n",
    "                     'Awaiting court outcome']:\n",
    "        return 'Non-criminal Outcome'\n",
    "    elif outcome in ['Further investigation is not in the public interest', \n",
    "                     'Further action is not in the public interest', \n",
    "                     'Formal action is not in the public interest']:\n",
    "        return 'Public Interest Consideration'\n",
    "    else:\n",
    "        return 'Unknown'  # Or any other category for unknown outcomes\n",
    "\n",
    "def apply_categorization(df):\n",
    "    \"\"\"\n",
    "    Apply categorization to 'Final Outcome' column.\n",
    "    \"\"\"\n",
    "    df['Broad Outcome Category'] = df['Final Outcome'].apply(categorize_outcome)\n",
    "    return df\n",
    "\n",
    "def del_values_street(df):\n",
    "    \"\"\"\n",
    "    Delete unnecessary columns from the DataFrame.\n",
    "    \"\"\"\n",
    "    cols_to_delete = ['Reported by', 'Context', 'Location', 'Last outcome category', 'Outcome type', 'Final Outcome']\n",
    "    df.drop(columns=cols_to_delete, inplace=True)\n",
    "    return df\n",
    "\n",
    "def stage_data(df, df_outcomes, output_file):\n",
    "    \"\"\"\n",
    "    Store the data to a CSV file for staging.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting data staging\")\n",
    "    try:\n",
    "        # Apply transformations\n",
    "        df = merge_data(df, df_outcomes)\n",
    "        df = finaloutcome(df)\n",
    "        df = apply_categorization(df)\n",
    "        df = del_values_street(df)\n",
    "\n",
    "        # Save to CSV\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logging.info(\"Data staging completed successfully\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during data staging: {e}\")\n",
    "\n",
    "def primary_transformations(df):\n",
    "    \"\"\"\n",
    "    Primary Storage Layer: Apply primary transformations to the data.\n",
    "    \"\"\"\n",
    "    # Example transformation: Convert some columns to categorical data type\n",
    "    df['Crime type'] = df['Crime type'].astype('category')\n",
    "    df['Broad Outcome Category'] = df['Broad Outcome Category'].astype('category')\n",
    "\n",
    "    # Example transformation: Create a new column by summing existing columns\n",
    "    if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
    "        df['Location Sum'] = df['Latitude'] + df['Longitude']\n",
    "\n",
    "    return df\n",
    "\n",
    "def primary_data(df, output_file):\n",
    "    \"\"\"\n",
    "    Primary Storage Layer: Store the primary transformed data to a CSV file.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting primary data transformation\")\n",
    "    try:\n",
    "        # Apply primary transformations\n",
    "        df = primary_transformations(df)\n",
    "\n",
    "        # Save to CSV\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logging.info(\"Primary data transformation completed successfully\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during primary data transformation: {e}\")\n",
    "\n",
    "def reporting_aggregation(df):\n",
    "    \"\"\"\n",
    "    Reporting Layer: Aggregate data for reporting purposes.\n",
    "    \"\"\"\n",
    "    # Example aggregation: Count of crimes by crime type and broad outcome category\n",
    "    agg_df = df.groupby(['Crime type', 'Broad Outcome Category']).size().reset_index(name='Count')\n",
    "\n",
    "    return agg_df\n",
    "\n",
    "def reporting_data(df, output_file):\n",
    "    \"\"\"\n",
    "    Reporting Layer: Store the aggregated reporting data to a CSV file.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting reporting data aggregation\")\n",
    "    try:\n",
    "        # Apply aggregation\n",
    "        agg_df = reporting_aggregation(df)\n",
    "\n",
    "        # Save to CSV\n",
    "        agg_df.to_csv(output_file, index=False)\n",
    "        logging.info(\"Reporting data aggregation completed successfully\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during reporting data aggregation: {e}\")\n",
    "\n",
    "def main():\n",
    "    logging.info(\"Pipeline execution started\")\n",
    "    try:\n",
    "        df = ingest_data(RAW_DATA_FILE)\n",
    "        df_outcomes = ingest_data(OUTCOMES_DATA_FILE)\n",
    "        \n",
    "        if df is not None and df_outcomes is not None:\n",
    "            stage_data(df, df_outcomes, STAGED_DATA_FILE)\n",
    "            df_staged = ingest_data(STAGED_DATA_FILE)  # Read the staged data for further processing\n",
    "            primary_data(df_staged, PRIMARY_DATA_FILE)\n",
    "            df_primary = ingest_data(PRIMARY_DATA_FILE)  # Read the primary data for reporting\n",
    "            reporting_data(df_primary, REPORTING_DATA_FILE)\n",
    "        logging.info(\"Pipeline execution completed successfully\")\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Pipeline execution failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
